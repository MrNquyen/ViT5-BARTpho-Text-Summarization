
dataset_attributes:
  annotations:
    # train: /datastore/npl/ViInfographicCaps/data/data/paddle_ocr_description/train_new.json
    # val: /datastore/npl/ViInfographicCaps/data/data/paddle_ocr_description/val_new.json
    # test: /datastore/npl/ViInfographicCaps/data/data/paddle_ocr_description/test_new.json
    train: /datastore/npl/ViInfographicCaps/data/data/vitern/train_dev_test/train_no_segmentation.json
    val: /datastore/npl/ViInfographicCaps/data/data/vitern/train_dev_test/val_no_segmentation.json
    test: /datastore/npl/ViInfographicCaps/data/data/vitern/train_dev_test/test_no_segmentation.json
model_attributes:
  pretrained: /datastore/npl/ViInfographicCaps/model/bartpho-syllable
  hidden_size: 768
  encoder:
    max_length: 128
    max_dec_length: 50
    return_tensors: pt
  decoder:
    max_length: 50
    num_layers: 4
    nhead: 12
    dropout: 0.1
    return_tensors: pt
  adjust_optimizer:
    lr_scale: 0.1 # scale lr for finetuning modules
optimizer_attributes:
  lr_scale: 0.1 # scale lr for finetuning modules
  params:
    eps: 1e-8
    lr: 1e-4 
    weight_decay: 0.01
  type: AdamW
training_parameters:
  epochs: 150
  batch_size: 64
  early_stopping:
    patience: 5
  lr_scheduler:
    status: true
    type: "step"
    gamma : 0.1
    lr_steps:
      - 3000
      - 6000
      - 8000
      - 10000
    lr_ratio: 0.1
    use_warmup: true
    warmup_factor: 0.5
    warmup_iterations: 1000
    lr_epoch_step_size: 5
    warmup_epochs: 10
    decay_factor: 0.3
  max_iterations: 12000
  snapshot_interval: 500
  snapshot_epoch_interval: 3
  monitored_metric: m4c_textcaps/textcaps_bleu4
  metric_minimize: false
  seed: 2021


    